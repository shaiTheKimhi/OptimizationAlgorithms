# OptimizationAlgorithms
This repository explores different Descent-based optimization algorithms and tests them over different functions including over a Neural Network.<br/>
The optimization algorithms are Gradient Descent, Newton Method and BFGS (L-BFGS, Quasi-Newton and conjugate gradients to be added..).<br/>
The functions optimized include:<br/>A basic quadratic function f(x) = xQx^t with custom matrix Q<br/>
Rosenbrock function<br/>
A Neural Network approximation function trying to approximate function f(x) = x*exp(-x1^2-x2^2)<br/>
The results figures can be seen at files "dnn and bfgs report.pdf" and "GD and Newton report.pdf"
